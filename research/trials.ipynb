{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Github repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Bappy\\\\Office\\\\Live-Session\\\\GenAI\\\\Source-Code-Analysis-using-GenAI\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'd:\\\\Bappy\\\\Office\\\\Live-Session\\\\GenAI\\\\Source-Code-Analysis-using-GenAI\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/entbappy/End-to-end-ML-Project-Implementation\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path+'/src/mlProject',\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import os\\nimport sys\\nimport logging\\n\\nlogging_str = \"[%(asctime)s: %(levelname)s: %(module)s: %(message)s]\"\\n\\nlog_dir = \"logs\"\\nlog_filepath = os.path.join(log_dir,\"running_logs.log\")\\nos.makedirs(log_dir, exist_ok=True)\\n\\n\\nlogging.basicConfig(\\n    level= logging.INFO,\\n    format= logging_str,\\n\\n    handlers=[\\n        logging.FileHandler(log_filepath),\\n        logging.StreamHandler(sys.stdout)\\n    ]\\n)\\n\\nlogger = logging.getLogger(\"mlProjectLogger\")', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport urllib.request as request\\nimport zipfile\\nfrom mlProject import logger\\nfrom mlProject.utils.common import get_size\\nfrom mlProject.entity.config_entity import DataIngestionConfig\\nfrom pathlib import Path\\n\\n\\nclass DataIngestion:\\n    def __init__(self, config: DataIngestionConfig):\\n        self.config = config\\n\\n    \\n    def download_file(self):\\n        if not os.path.exists(self.config.local_data_file):\\n            filename, headers = request.urlretrieve(\\n                url = self.config.source_URL,\\n                filename = self.config.local_data_file\\n            )\\n            logger.info(f\"{filename} download! with following info: \\\\n{headers}\")\\n        else:\\n            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")\\n\\n    \\n    def extract_zip_file(self):\\n        \"\"\"\\n        zip_file_path: str\\n        Extracts the zip file into the data directory\\n        Function returns None\\n        \"\"\"\\n        unzip_path = self.config.unzip_dir\\n        os.makedirs(unzip_path, exist_ok=True)\\n        with zipfile.ZipFile(self.config.local_data_file, \\'r\\') as zip_ref:\\n            zip_ref.extractall(unzip_path)\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom mlProject import logger\\nfrom sklearn.model_selection import train_test_split\\nimport pandas as pd\\nfrom mlProject.entity.config_entity import DataTransformationConfig\\n\\n\\nclass DataTransformation:\\n    def __init__(self, config: DataTransformationConfig):\\n        self.config = config\\n\\n    \\n    ## Note: You can add different data transformation techniques such as Scaler, PCA and all\\n    #You can perform all kinds of EDA in ML cycle here before passing this data to the model\\n\\n    # I am only adding train_test_spliting cz this data is already cleaned up\\n\\n\\n    def train_test_spliting(self):\\n        data = pd.read_csv(self.config.data_path)\\n\\n        # Split the data into training and test sets. (0.75, 0.25) split.\\n        train, test = train_test_split(data)\\n\\n        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False)\\n        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\\n\\n        logger.info(\"Splited data into training and test sets\")\\n        logger.info(train.shape)\\n        logger.info(test.shape)\\n\\n        print(train.shape)\\n        print(test.shape)\\n        ', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom mlProject import logger\\nimport pandas as pd\\nfrom mlProject.entity.config_entity import DataValidationConfig\\n                                    \\n\\n\\n\\nclass DataValiadtion:\\n    def __init__(self, config: DataValidationConfig):\\n        self.config = config\\n\\n\\n    def validate_all_columns(self)-> bool:\\n        try:\\n            validation_status = None\\n\\n            data = pd.read_csv(self.config.unzip_data_dir)\\n            all_cols = list(data.columns)\\n\\n            all_schema = self.config.all_schema.keys()\\n\\n            \\n            for col in all_cols:\\n                if col not in all_schema:\\n                    validation_status = False\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n                else:\\n                    validation_status = True\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n\\n            return validation_status\\n        \\n        except Exception as e:\\n            raise e\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport pandas as pd\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nfrom mlProject.utils.common import save_json\\nfrom urllib.parse import urlparse\\nimport numpy as np\\nimport joblib\\nfrom mlProject.entity.config_entity import ModelEvaluationConfig\\nfrom pathlib import Path\\n\\n\\nclass ModelEvaluation:\\n    def __init__(self, config: ModelEvaluationConfig):\\n        self.config = config\\n\\n    \\n    def eval_metrics(self,actual, pred):\\n        rmse = np.sqrt(mean_squared_error(actual, pred))\\n        mae = mean_absolute_error(actual, pred)\\n        r2 = r2_score(actual, pred)\\n        return rmse, mae, r2\\n    \\n\\n\\n    def save_results(self):\\n\\n        test_data = pd.read_csv(self.config.test_data_path)\\n        model = joblib.load(self.config.model_path)\\n\\n        test_x = test_data.drop([self.config.target_column], axis=1)\\n        test_y = test_data[[self.config.target_column]]\\n        \\n        predicted_qualities = model.predict(test_x)\\n\\n        (rmse, mae, r2) = self.eval_metrics(test_y, predicted_qualities)\\n        \\n        # Saving metrics as local\\n        scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\\n        save_json(path=Path(self.config.metric_file_name), data=scores)\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import pandas as pd\\nimport os\\nfrom mlProject import logger\\nfrom sklearn.linear_model import ElasticNet\\nimport joblib\\nfrom mlProject.entity.config_entity import ModelTrainerConfig\\n\\n\\n\\nclass ModelTrainer:\\n    def __init__(self, config: ModelTrainerConfig):\\n        self.config = config\\n\\n    \\n    def train(self):\\n        train_data = pd.read_csv(self.config.train_data_path)\\n        test_data = pd.read_csv(self.config.test_data_path)\\n\\n\\n        train_x = train_data.drop([self.config.target_column], axis=1)\\n        test_x = test_data.drop([self.config.target_column], axis=1)\\n        train_y = train_data[[self.config.target_column]]\\n        test_y = test_data[[self.config.target_column]]\\n\\n\\n        lr = ElasticNet(alpha=self.config.alpha, l1_ratio=self.config.l1_ratio, random_state=42)\\n        lr.fit(train_x, train_y)\\n\\n        joblib.dump(lr, os.path.join(self.config.root_dir, self.config.model_name))\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.constants import *\\nfrom mlProject.utils.common import read_yaml, create_directories\\nfrom mlProject.entity.config_entity import (DataIngestionConfig,\\n                                            DataValidationConfig,\\n                                            DataTransformationConfig,\\n                                            ModelTrainerConfig,\\n                                            ModelEvaluationConfig)\\n\\nclass ConfigurationManager:\\n    def __init__(\\n        self,\\n        config_filepath = CONFIG_FILE_PATH,\\n        params_filepath = PARAMS_FILE_PATH,\\n        schema_filepath = SCHEMA_FILE_PATH):\\n\\n        self.config = read_yaml(config_filepath)\\n        self.params = read_yaml(params_filepath)\\n        self.schema = read_yaml(schema_filepath)\\n\\n        create_directories([self.config.artifacts_root])\\n        \\n\\n    def get_data_ingestion_config(self) -> DataIngestionConfig:\\n        config = self.config.data_ingestion\\n\\n        create_directories([config.root_dir])\\n\\n        data_ingestion_config = DataIngestionConfig(\\n            root_dir=config.root_dir,\\n            source_URL=config.source_URL,\\n            local_data_file=config.local_data_file,\\n            unzip_dir=config.unzip_dir \\n        )\\n\\n        return data_ingestion_config\\n    \\n\\n    \\n    def get_data_validation_config(self) -> DataValidationConfig:\\n        config = self.config.data_validation\\n        schema = self.schema.COLUMNS\\n\\n        create_directories([config.root_dir])\\n\\n        data_validation_config = DataValidationConfig(\\n            root_dir=config.root_dir,\\n            STATUS_FILE=config.STATUS_FILE,\\n            unzip_data_dir = config.unzip_data_dir,\\n            all_schema=schema,\\n        )\\n\\n        return data_validation_config\\n    \\n\\n    \\n    def get_data_transformation_config(self) -> DataTransformationConfig:\\n        config = self.config.data_transformation\\n\\n        create_directories([config.root_dir])\\n\\n        data_transformation_config = DataTransformationConfig(\\n            root_dir=config.root_dir,\\n            data_path=config.data_path,\\n        )\\n\\n        return data_transformation_config\\n    \\n\\n    def get_model_trainer_config(self) -> ModelTrainerConfig:\\n        config = self.config.model_trainer\\n        params = self.params.ElasticNet\\n        schema =  self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_trainer_config = ModelTrainerConfig(\\n            root_dir=config.root_dir,\\n            train_data_path = config.train_data_path,\\n            test_data_path = config.test_data_path,\\n            model_name = config.model_name,\\n            alpha = params.alpha,\\n            l1_ratio = params.l1_ratio,\\n            target_column = schema.name\\n            \\n        )\\n\\n        return model_trainer_config\\n    \\n\\n\\n    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\\n        config = self.config.model_evaluation\\n        params = self.params.ElasticNet\\n        schema =  self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_evaluation_config = ModelEvaluationConfig(\\n            root_dir=config.root_dir,\\n            test_data_path=config.test_data_path,\\n            model_path = config.model_path,\\n            all_params=params,\\n            metric_file_name = config.metric_file_name,\\n            target_column = schema.name\\n           \\n        )\\n\\n        return model_evaluation_config', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\config\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\n\\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\\nPARAMS_FILE_PATH = Path(\"params.yaml\")\\nSCHEMA_FILE_PATH = Path(\"schema.yaml\")', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from dataclasses import dataclass\\nfrom pathlib import Path\\n\\n\\n@dataclass(frozen=True)\\nclass DataIngestionConfig:\\n    root_dir: Path\\n    source_URL: str\\n    local_data_file: Path\\n    unzip_dir: Path\\n\\n\\n\\n@dataclass(frozen=True)\\nclass DataValidationConfig:\\n    root_dir: Path\\n    STATUS_FILE: str\\n    unzip_data_dir: Path\\n    all_schema: dict\\n\\n\\n\\n@dataclass(frozen=True)\\nclass DataTransformationConfig:\\n    root_dir: Path\\n    data_path: Path\\n\\n\\n\\n@dataclass(frozen=True)\\nclass ModelTrainerConfig:\\n    root_dir: Path\\n    train_data_path: Path\\n    test_data_path: Path\\n    model_name: str\\n    alpha: float\\n    l1_ratio: float\\n    target_column: str\\n\\n\\n@dataclass(frozen=True)\\nclass ModelEvaluationConfig:\\n    root_dir: Path\\n    test_data_path: Path\\n    model_path: Path\\n    all_params: dict\\n    metric_file_name: Path\\n    target_column: str', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\entity\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"import joblib \\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\n\\n\\nclass PredictionPipeline:\\n    def __init__(self):\\n        self.model = joblib.load(Path('artifacts/model_trainer/model.joblib'))\\n\\n    \\n    def predict(self, data):\\n        prediction = self.model.predict(data)\\n\\n        return prediction\", metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_ingestion import DataIngestion\\nfrom mlProject import logger\\n\\nSTAGE_NAME = \"Data Ingestion stage\"\\n\\n\\nclass DataIngestionTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_ingestion_config = config.get_data_ingestion_config()\\n        data_ingestion = DataIngestion(config=data_ingestion_config)\\n        data_ingestion.download_file()\\n        data_ingestion.extract_zip_file()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataIngestionTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_01_data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_validation import DataValiadtion\\nfrom mlProject import logger\\n\\n\\nSTAGE_NAME = \"Data Validation stage\"\\n\\nclass DataValidationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_validation_config = config.get_data_validation_config()\\n        data_validation = DataValiadtion(config=data_validation_config)\\n        data_validation.validate_all_columns()\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataValidationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_02_data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_transformation import DataTransformation\\nfrom mlProject import logger\\nfrom pathlib import Path\\n\\n\\nSTAGE_NAME = \"Data Transformation stage\"\\n\\nclass DataTransformationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n\\n    def main(self):\\n        try:\\n            with open(Path(\"artifacts/data_validation/status.txt\"), \"r\") as f:\\n                status = f.read().split(\" \")[-1]\\n\\n            if status == \"True\":\\n                config = ConfigurationManager()\\n                data_transformation_config = config.get_data_transformation_config()\\n                data_transformation = DataTransformation(config=data_transformation_config)\\n                data_transformation.train_test_spliting()\\n\\n            else:\\n                raise Exception(\"You data schema is not valid\")\\n\\n        except Exception as e:\\n            print(e)\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_03_data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_trainer import ModelTrainer\\nfrom mlProject import logger\\n\\n\\n\\nSTAGE_NAME = \"Model Trainer stage\"\\n\\nclass ModelTrainerTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        model_trainer_config = config.get_model_trainer_config()\\n        model_trainer_config = ModelTrainer(config=model_trainer_config)\\n        model_trainer_config.train()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelTrainerTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_04_model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_evaluation import ModelEvaluation\\nfrom mlProject import logger\\n\\n\\nSTAGE_NAME = \"Model evaluation stage\"\\n\\nclass ModelEvaluationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        model_evaluation_config = config.get_model_evaluation_config()\\n        model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\\n        model_evaluation_config.save_results()\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelEvaluationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_05_model_evaluation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom box.exceptions import BoxValueError\\nimport yaml\\nfrom mlProject import logger\\nimport json\\nimport joblib\\nfrom ensure import ensure_annotations\\nfrom box import ConfigBox\\nfrom pathlib import Path\\nfrom typing import Any\\n\\n\\n\\n@ensure_annotations\\ndef read_yaml(path_to_yaml: Path) -> ConfigBox:\\n    \"\"\"reads yaml file and returns\\n\\n    Args:\\n        path_to_yaml (str): path like input\\n\\n    Raises:\\n        ValueError: if yaml file is empty\\n        e: empty file\\n\\n    Returns:\\n        ConfigBox: ConfigBox type\\n    \"\"\"\\n    try:\\n        with open(path_to_yaml) as yaml_file:\\n            content = yaml.safe_load(yaml_file)\\n            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\\n            return ConfigBox(content)\\n    except BoxValueError:\\n        raise ValueError(\"yaml file is empty\")\\n    except Exception as e:\\n        raise e\\n    \\n\\n\\n@ensure_annotations\\ndef create_directories(path_to_directories: list, verbose=True):\\n    \"\"\"create list of directories\\n\\n    Args:\\n        path_to_directories (list): list of path of directories\\n        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\\n    \"\"\"\\n    for path in path_to_directories:\\n        os.makedirs(path, exist_ok=True)\\n        if verbose:\\n            logger.info(f\"created directory at: {path}\")\\n\\n\\n@ensure_annotations\\ndef save_json(path: Path, data: dict):\\n    \"\"\"save json data\\n\\n    Args:\\n        path (Path): path to json file\\n        data (dict): data to be saved in json file\\n    \"\"\"\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logger.info(f\"json file saved at: {path}\")\\n\\n\\n\\n\\n@ensure_annotations\\ndef load_json(path: Path) -> ConfigBox:\\n    \"\"\"load json files data\\n\\n    Args:\\n        path (Path): path to json file\\n\\n    Returns:\\n        ConfigBox: data as class attributes instead of dict\\n    \"\"\"\\n    with open(path) as f:\\n        content = json.load(f)\\n\\n    logger.info(f\"json file loaded succesfully from: {path}\")\\n    return ConfigBox(content)\\n\\n\\n@ensure_annotations\\ndef save_bin(data: Any, path: Path):\\n    \"\"\"save binary file\\n\\n    Args:\\n        data (Any): data to be saved as binary\\n        path (Path): path to binary file\\n    \"\"\"\\n    joblib.dump(value=data, filename=path)\\n    logger.info(f\"binary file saved at: {path}\")\\n\\n\\n@ensure_annotations\\ndef load_bin(path: Path) -> Any:\\n    \"\"\"load binary data\\n\\n    Args:\\n        path (Path): path to binary file\\n\\n    Returns:\\n        Any: object stored in the file\\n    \"\"\"\\n    data = joblib.load(path)\\n    logger.info(f\"binary file loaded from: {path}\")\\n    return data\\n\\n\\n\\n@ensure_annotations\\ndef get_size(path: Path) -> str:\\n    \"\"\"get size in KB\\n\\n    Args:\\n        path (Path): path of the file\\n\\n    Returns:\\n        str: size in KB\\n    \"\"\"\\n    size_in_kb = round(os.path.getsize(path)/1024)\\n    return f\"~ {size_in_kb} KB\"\\n\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 2000,\n",
    "                                                             chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"***************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge base (vector DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./data')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is DataIngestion class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 19, updating n_results = 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `DataIngestion` class is to handle the downloading and extraction of files based on the provided configuration. It is part of the Data Ingestion stage in the training pipeline and performs tasks such as downloading files and extracting zip files.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
